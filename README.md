# Awesome-VLM-Architectures
| Paper Title                                | Architecture Overview                                                                                                                                                                                                                                   | Training Method Detailed                                                                                                                                                                                               | Alignment techniques                                                                                                                                                                   | Datasets Used                         |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| LLaVA: Large Language and Vision Assistant | LLaVA connects a vision encoder and an LLM for general-purpose visual and language understanding. It uses the open-set visual encoder of CLIP with the language decoder Vicuna, fine-tuning end-to-end on generated instructional vision-language data. | LLaVA is trained in two stages: pre-training for feature alignment using filtered CC3M image-text pairs, and fine-tuning end-to-end on LLaVA-Instruct-158K dataset for multimodal chatbot and Science QA applications. | A simple linear layer connects image features into the word embedding space of the LLM. The model uses auto-regressive training objective for instruction-tuning on prediction tokens. | CC3M, LLaVA-Instruct-158K, Science QA |
