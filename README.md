# Awesome-VLM-Architectures
<details> 
  <summary><h3>LLaVA</h3></summary> 
  
| title                                                                                 | architecture.overview                                                                                                   | architecture.components                                                                        | training.methods                                                                                                                                                                      | alignment_techniques.methods                                                                                                                               | datasets.used                                                                    |
| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) | Enhanced LLaVA with a two-layer MLP for the vision-language connector and inclusion of academic-task-oriented VQA data. | Vision Encoder (CLIP ViT-L/336px), Language Model (Vicuna 13B), MLP vision-language connector. | Two-stage training with pre-training on LCS-558K dataset and fine-tuning on a mixture of VQA, OCR, and region-level VQA datasets, resulting in a final training dataset size of 665K. | Employment of a two-layer MLP for improved multimodal representation and inclusion of academic-task-oriented VQA datasets for enhanced model capabilities. | LCS-558K, VQA, OCR, region-level VQA, visual conversation, language conversation |
</details>
<details>  
  <summary><h3>LLaVA 1.5</h3></summary> 
  
  | title                                                                                 | model_name | architecture.overview                                                                                                   | architecture.components                                                                        | training.methods                                                                                                                                                                      | alignment_techniques.methods                                                                                                                               | datasets.used                                                                    |
| ------------------------------------------------------------------------------------- | ---------- | ----------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) | LLaVA-1.5  | Enhanced LLaVA with a two-layer MLP for the vision-language connector and inclusion of academic-task-oriented VQA data. | Vision Encoder (CLIP ViT-L/336px), Language Model (Vicuna 13B), MLP vision-language connector. | Two-stage training with pre-training on LCS-558K dataset and fine-tuning on a mixture of VQA, OCR, and region-level VQA datasets, resulting in a final training dataset size of 665K. | Employment of a two-layer MLP for improved multimodal representation and inclusion of academic-task-oriented VQA datasets for enhanced model capabilities. | LCS-558K, VQA, OCR, region-level VQA, visual conversation, language conversation |
</details>

